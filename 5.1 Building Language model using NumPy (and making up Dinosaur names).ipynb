{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RNN using NumPy\n",
    "\n",
    "* Coding a RNN **from scratch in numpy** - including optimization using gradient descent and clipped gradients\n",
    "* Dinosaur names - training a character level language model and sampling new names\n",
    "* Using a pretrained model to generate Shakerpearean poem \n",
    "---\n",
    "\n",
    "### Language Model\n",
    "\n",
    "* At each time-step, the RNN tries to predict what the next character is, given the previous characters. \n",
    "* $\\mathbf{X} = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a list of characters from the training set.\n",
    "* $\\mathbf{Y} = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is the same list of characters but shifted one character forward. \n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Figure 1</b>: Recurrent Neural Network </center></caption></img>\n",
    "\n",
    "### Sampling a New Sequence\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Figure 3</b>:Pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time-step, and have the network sample one character at a time. </center></caption></img>\n",
    "\n",
    "*hidden state:*  \n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
    "*activation:*\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n",
    "*prediction:*\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n",
    "where $x^{\\langle 1 \\rangle} = \\vec{0}$ and $a^{\\langle 0 \\rangle} = \\vec{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling a Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters from a trained RNN\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and shapes\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # Initialize 'x' and 'a' vectors\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # List to hold the output\n",
    "    indices = []   \n",
    "    \n",
    "    # Stopping condition - length of 50 or newline character\n",
    "    idx = -1       # Initialize idx to non-zero (zero holds newline character)\n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Forward propagate\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "\n",
    "        # Pick a character\n",
    "        idx = np.random.choice(range(len(y.ravel())), p=y.ravel())\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Update values for next step\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        a_prev = a\n",
    "        \n",
    "        counter +=1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    name = ''.join([ix_to_char[i] for i in indices])\n",
    "    name = name[0].upper() + name[1:]\n",
    "    \n",
    "    return indices, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping Gradients\n",
    "\n",
    "* Exploding gradients make training difficult, because the updates may be so large that they \"overshoot\" the optimal values during back propagation.\n",
    "* Before updating the parameters, we will perform gradient clipping\n",
    "\n",
    "\n",
    "<img src=\"images/clip.png\" style=\"width:400;height:150px;\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, val):\n",
    "    '''\n",
    "    Clips the gradients' values b/w provided limit.\n",
    "    '''\n",
    "    gradients = copy.deepcopy(gradients)\n",
    "    \n",
    "    dWaa, dWax, dWya, dba, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['dba'], gradients['dby']\n",
    "   \n",
    "    for gradient in [dWaa, dWax, dWya, dba, dby]:\n",
    "        np.clip(gradient, -val, val, out = gradient)\n",
    "\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"dba\": dba, \"dby\": dby}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward & Backward Propagation\n",
    "\n",
    "**Dimensions**:\n",
    "* Wax -- Weight matrix for the input (xt), numpy array of shape (n_a, n_x)\n",
    "* Waa -- Weight matrix for the hidden state (at), numpy array of shape (n_a, n_a)\n",
    "* Wya -- Weight matrix for activations to the output, numpy array of shape (n_y, n_a)\n",
    "* ba -- Bias for hidden state, numpy array of shape (n_a, 1)\n",
    "* by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "* xt of shape (n_x, m).\n",
    "* at of shape (n_a, m)\n",
    "* yt of shape (n_y, m)\n",
    "\n",
    "#### **NOTE 1**:\n",
    "* We don't cast X into (n_x, m, T_x) tensor, and train over each sample individually, where each sample has a diff. length.\n",
    "* Instead of converting X into One-Hot vector matrix at the beginning, we handle it in Forward Pass step\n",
    "\n",
    "#### **NOTE 2**:\n",
    "* Gradients are updated for each alphabet in the word (or each word in the sentence)\n",
    "* However, Parameters are updates only 1x per word (in this case) or (more generally) 1x/batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "    ba = np.zeros((n_a, 1)) # hidden bias\n",
    "    by = np.zeros((n_y, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba,\"by\": by}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(parameters, a_prev, xt):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell\n",
    "    \"\"\"\n",
    "    Wax, Waa, Wya = parameters[\"Wax\"], parameters[\"Waa\"], parameters[\"Wya\"]\n",
    "    ba, by = parameters[\"ba\"], parameters[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by)\n",
    "\n",
    "    #cache = (a_next, a_prev, xt, parameters)    # dict is built in rnn_forward function instead\n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters, n_x = 27):\n",
    "    \"\"\"\n",
    "    Implements one full forward pass\n",
    "    \"\"\"\n",
    "    # Initialize x, a and y_hat as dictionaries to hold values for each step\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0)\n",
    "    loss = 0             # initialize your loss to 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        # ONE-HOT VECTOR TRANSFORMATION\n",
    "        val = X[t]\n",
    "        x[t] = np.zeros((n_x,1)) \n",
    "        if (val != None):\n",
    "            x[t][val] = 1\n",
    "        \n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        \n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t],0])   # y*log(y_hat) = log( y_hat[relevant_index] )\n",
    "        \n",
    "    cache = (y_hat, a, x)     \n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['dba'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \n",
    "    # Retrieve from cache and parameters\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    \n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "    \n",
    "    # Initializing gradients\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['dba'], gradients['dby'] = np.zeros_like(ba), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    # Backpropagate through time\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, lr):\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['ba']  += -lr * gradients['dba']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    X -- one example word representated as a list of integers\n",
    "    Y -- exactly the same as X but shifted one index to the left.\n",
    "    \n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    loss, cache  = rnn_forward(X, Y, a_prev, parameters)                   # loss is calculated for each alphabet\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)                   # gradients are updated for each alphabet\n",
    "    gradients    = clip(gradients, 5)\n",
    "    parameters   = update_parameters(parameters, gradients, learning_rate) # one step of update takes place\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dinosaur Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/rnn/dinos.txt', 'r').read()\n",
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  27\n",
      "Characters:  ['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print('Vocab size: ', vocab_size)\n",
    "print('Characters: ', chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aachenosaurus', 'aardonyx', 'abdallahsaurus', 'abelisaurus', 'abrictosaurus']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.split('\\n')\n",
    "data = [line.strip() for line in data]\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'a', 'c', 'h', 'e', 'n', 'o', 's', 'a', 'u', 'r', 'u', 's'],\n",
       " ['a', 'a', 'r', 'd', 'o', 'n', 'y', 'x']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ch = [list(name) for name in data]\n",
    "data_ch[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(data_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['t', 'u', 'r', 'i', 'a', 's', 'a', 'u', 'r', 'u', 's']] \n",
      " [[20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19], [16, 1, 14, 4, 15, 18, 1, 22, 5, 14, 1, 20, 15, 18]]\n"
     ]
    }
   ],
   "source": [
    "data_int = []\n",
    "for i in range(len(data_ch)):\n",
    "    data_int.append([char_to_ix[c] for c in data_ch[i]])\n",
    "    \n",
    "print(data_ch[:1], '\\n', data_int[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Generating New Names (after Training Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data_x -- text corpus (list of integers)\n",
    "    ix_to_char -- maps the index to a character\n",
    "    char_to_ix -- maps a character to an index\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names to sample at each iteration. \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize shapes, parameters\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    m = len(data_x)\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss)\n",
    "    loss = -np.log(1.0/vocab_size)*dino_names\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        # Select an example and set X & Y\n",
    "        idx = j % m\n",
    "        X = [None] + data_x[idx]\n",
    "        Y = data_x[idx] + [char_to_ix['\\n']]\n",
    "\n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        # to keep the loss smooth.\n",
    "        loss = loss*0.999 + curr_loss*0.001\n",
    "\n",
    "        # Check every 2000 Iteration\n",
    "        if (j > 0) and (j % 5000 == 0):  \n",
    "            print('Iteration: {}, Loss: {:.4f}'.format(j, loss) + '\\n')\n",
    "            \n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                sampled_ix, sampled_name = sample(parameters, char_to_ix, seed)\n",
    "                print(sampled_name.replace('\\n',''))  # removes extra line\n",
    "                seed += 1\n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5000, Loss: 25.2018\n",
      "\n",
      "Encaradudon\n",
      "Eosaurus\n",
      "Noopeodhenwanrmolongastia\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.8439\n",
      "\n",
      "Annabverathaurorhatophnimidraithomys\n",
      "Lolomalesmphocenioodenitan\n",
      "Palsiia\n",
      "\n",
      "\n",
      "Iteration: 15000, Loss: 23.0549\n",
      "\n",
      "Erathosaurus\n",
      "Banopalren\n",
      "Pholonobor\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 23.0673\n",
      "\n",
      "Teohushdewlopestyluguantosaurus\n",
      "Stoceshaegstroclolisaurus\n",
      "Mopachanglosaurus\n",
      "\n",
      "\n",
      "Iteration: 25000, Loss: 22.7241\n",
      "\n",
      "Agngnrang\n",
      "Oluasaurus\n",
      "Mantasia\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 22.7230\n",
      "\n",
      "Inchusaurus\n",
      "Licampalisaurus\n",
      "S\n",
      "\n",
      "\n",
      "Iteration: 35000, Loss: 22.3328\n",
      "\n",
      "Henreosaurus\n",
      "Ychosaurus\n",
      "Chadiys\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = model(data_int, ix_to_char, char_to_ix, 35001, verbose = True, dino_names=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Shakespeare Sonet Example\n",
    "\n",
    "A similar task to character-level text generation (but more complicated) is generating Shakespearean poems. Instead of learning from a dataset of dinosaur names, we can use a collection of Shakespearean poems. \n",
    "\n",
    "Using LSTM cells, we can learn longer-term dependencies that span many characters in the text--e.g., where a character appearing somewhere a sequence can influence what should be a different character, much later in the sequence.\n",
    "\n",
    "<img src=\"images/shakespeare.jpg\" style=\"width:500;height:400px;\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 94275\n",
      "s rose might never die,\n",
      "but as the riper should by time decease,\n",
      "his tender heir\n"
     ]
    }
   ],
   "source": [
    "text = io.open('data/rnn/shakespeare.txt', encoding='utf-8').read().lower()\n",
    "print('corpus length:', len(text))\n",
    "print(text[100:180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters in the corpus: 38\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print('number of unique characters in the corpus:', len(chars))   # becomes n_x\n",
    "\n",
    "n_x = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(text, Tx = 40, stride = 3):\n",
    "    \"\"\"\n",
    "    Create a training set by scanning a window of size Tx over the text corpus, with stride 3.\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for i in range(0, len(text) - Tx, stride):\n",
    "        X.append(text[i: i + Tx])\n",
    "        Y.append(text[i + Tx])    # next alphabet in the sequence\n",
    "    \n",
    "    print('number of training examples:', len(X))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(X, Y, n_x, char_indices, Tx = 40):\n",
    "    \"\"\"\n",
    "    Convert X and Y (lists) into arrays to be given to a recurrent neural network.\n",
    "    \n",
    "    Returns:\n",
    "    x -- array of shape (m, Tx, n_x)\n",
    "    y -- array of shape (m, n_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X)\n",
    "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
    "    y = np.zeros((m, n_x), dtype=np.bool)\n",
    "    for i, sentence in enumerate(X):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[Y[i]]] = 1\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 31412\n",
      "31412 31412\n"
     ]
    }
   ],
   "source": [
    "Tx = 40\n",
    "stride  = 3\n",
    "\n",
    "X, Y = build_data(text, Tx, stride)\n",
    "print(len(X), len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the sonnets\\n\\nby william shakespeare\\n\\nfro',\n",
       "  ' sonnets\\n\\nby william shakespeare\\n\\nfrom f'],\n",
       " ['m', 'a'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2], Y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31412, 40, 38) (31412, 38)\n"
     ]
    }
   ],
   "source": [
    "x, y = vectorization(X, Y, n_x, char_indices = char_indices) \n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 35s 141ms/step - loss: 2.5554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b8124dec08>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('pretrainedmodel/rnn_shakespeare/model_shakespeare_kiank_350_epoch.h5')\n",
    "model.fit(x, y, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample1(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    out = np.random.choice(range(n_x), p = probas.ravel())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem():\n",
    "    usr_input = input(\"Write the beginning for a poem, and let Shakespeare complete it: \")\n",
    "    \n",
    "    # zero pad the sentence to Tx characters.\n",
    "    sentence = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
    "    print('Padded sentence: ', sentence)\n",
    "    \n",
    "    generated = usr_input \n",
    "\n",
    "    print(\"\\n\\nHere is your poem (next 400 characters): \\n\\n\") \n",
    "    print(usr_input)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, Tx, n_x))\n",
    "\n",
    "        for t, char in enumerate(sentence):\n",
    "            if char != '0':\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "        # Generate prediction for next character\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample1(preds, temperature = 1.0)\n",
    "        next_char = indices_char[next_index]\n",
    "        \n",
    "        # Add that to the sentence\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning for a poem, and let Shakespeare complete it: Thou lips are like a summer breeze,\n",
      "Padded sentence:  00000thou lips are like a summer breeze,\n",
      "\n",
      "\n",
      "Here is your poem (next 400 characters): \n",
      "\n",
      "\n",
      "Thou lips are like a summer breeze,\n",
      " toof shen formited,\n",
      "shanth pout deder's wiet-trom bnhound my eveny from love,\n",
      "thy fore misein  in that comcost to,,\n",
      "that hit try frormar wastes of my keans being,\n",
      "youren to the mray me your cruth in they see,\n",
      "shale the mvay of this,  all dine newil,\n",
      "not sanse and i strowmed wuld, cortet i sind\n",
      "and dos the prairor wind,\n",
      "is goeteds a sach fros as forsed,\n",
      "thus and plerannery tompary nehed,\n",
      "who nef m"
     ]
    }
   ],
   "source": [
    "generate_poem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coursera]",
   "language": "python",
   "name": "conda-env-coursera-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
