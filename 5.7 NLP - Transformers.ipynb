{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network\n",
    "A neural network that takes advantage of parallel processing and substantially speeds up the training process. We first build functions for individual pieces, then tie them all together.  \n",
    "\n",
    "**Full Encoder** (`Encoder`) = Word Embedded Input + Positional Encoding + Dropout + (Multiple) Encoder Layers\n",
    "- Positional Encoding (`get_angles -> positional_encoding`) - returns a matrix (1, input_positions, position_encoding_length)   \n",
    "- Encoder Layer (`EncoderLayer`) - combines MultiHeadAttention + LayerNorm + FFN + Dropout + LayerNorm\n",
    "- Padding Mask (`create_padding_mask`) - so that padded values of 0, get -1e9 before feeding into softmax layer  \n",
    "- Look ahead Mask (`create_look_ahead_mask`) - to pretend during training that the entire output sequence is not known  \n",
    "- Self-Attention (`scaled_dot_product_attention`) - creates the self-attention vectors $A(Q, K,V)$   \n",
    "\n",
    "**Full Decoder** (`Decoder`) = Word Embedded Input + Positional Encoding + (Multiple) Decoder Layers\n",
    "- Decoder Layer (`DecoderLayer`)- combines 2 MultiHeadAttention layers + LayerNorm + FFN\n",
    "\n",
    "<img src=\"images/transformer.png\" alt=\"Transformer\" width=\"550\"/>\n",
    "<caption><center><font color='purple'><b>Transformer Encoder & Decoder</font></center></caption></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n",
    "# from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
    "# from transformers import TFDistilBertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Positional Encoding\n",
    "\n",
    "In sequence to sequence tasks, the relative order of data is extremely important to its meaning. When we were training sequential neural networks such as RNNs, we fed the inputs into the network in order. Information about the order of your data was automatically fed into your model. However, in a Transformer network using multi-head attention, data is fed all at once and *there is no information about the order of the data.* This is where positional encoding is useful - we can encode the positions of our inputs and pass them into the network using these sine and cosine formulas:\n",
    "    \n",
    "The sum of the positional encoding and word embedding is ultimately what is fed into the model. If we just hard code the positions in, say by adding a matrix of 1's or whole numbers to the word embedding, the semantic meaning is distorted. Conversely, the values of the sine and cosine equations are small enough (between -1 and 1) that when we add the positional encoding to a word embedding, the word embedding is not significantly distorted, and is instead enriched with positional information. \n",
    "\n",
    "*(In the lectures Andrew uses vertical vectors, but in this code all vectors are horizontal. All matrix multiplications should be adjusted accordingly.)*\n",
    "\n",
    "$$PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)$$\n",
    "\n",
    "* $d$ is the dimension of the word embedding and positional encoding\n",
    "* $pos$ is the position of the word.\n",
    "* $k$ refers to each of the different dimensions in the positional encodings, with $i$ equal to $k$ $//$ $2$.\n",
    "\n",
    "Notice that the inner terms for both equations are the same. E.g., angles for $PE_{(pos, 2)}$ and $PE_{(pos, 3)}$ are same.  \n",
    "$$\\theta(pos, i, d) = \\frac{pos}{10000^{\\frac{2i}{d}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_angles()` to calculate the possible angles for the sine and cosine positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, k, d):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "    \n",
    "    Arguments:\n",
    "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "        k --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
    "            # k refers to each of the different dimensions in the positional encodings\n",
    "        d(integer) -- Encoding size\n",
    "    \n",
    "    Returns:\n",
    "        angles -- (pos, d) numpy array \n",
    "    \"\"\"\n",
    "    # Get i from dimension span k\n",
    "    i = k//2\n",
    "    # Calculate the angles using pos, i and d\n",
    "    angles = pos/(10000**(2*i/d))    \n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position = 2\n",
    "# d_model = 4\n",
    "# pos_m = np.arange(position)[:, np.newaxis]\n",
    "# dims = np.arange(d_model)[np.newaxis, :]\n",
    "# get_angles(pos_m, dims, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    positions (int) -- Maximum number of positions to be encoded \n",
    "    d (int) -- Encoding size \n",
    "    pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "    # initialize a matrix angle_rads of all the angles \n",
    "    angle_rads = get_angles(np.arange(positions)[:, np.newaxis],  # pos\n",
    "                            np.arange(d)[np.newaxis, :],          # k\n",
    "                            d)                                    # d\n",
    "  \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Masking\n",
    "\n",
    "There are two types of masks that are useful when building a Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in the input sentence. \n",
    "\n",
    "### Padding Mask\n",
    "\n",
    "Oftentimes the input sequence will exceed the maximum length of a sequence our network can process. Let's say the maximum length of our model is five, it is fed the following sequences:\n",
    "\n",
    "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"], \n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
    "     [\"Exciting\", \"!\"]\n",
    "    ]\n",
    "\n",
    "which might get vectorized as:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600]\n",
    "    ]\n",
    "    \n",
    "When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99],\n",
    "     [ 2344, 345, 1284, 15, 0],    <-- Note, truncated part included as a new vector\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600, 0, 0, 0],\n",
    "    ]\n",
    "    \n",
    "Sequences longer than the maximum length of five will be truncated, and zeros will be added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, zeros will also be added for padding. However, these zeros will affect the softmax calculation - this is when a padding mask comes in handy! We will need to define a **boolean mask** that specifies which elements we must **attend(1)** and which elements you must **ignore(0)**. Later we will use that mask to set all the zeros in the sequence to a value close to negative infinity (-1e9).\n",
    "\n",
    "After masking, our input should go from `[87, 600, 0, 0, 0]` to `[87, 600, -1e9, -1e9, -1e9]`, so that when we take the softmax, the zeros don't affect the score. (exp(-1e9) ~ 0)\n",
    "\n",
    "The [MultiheadAttention](https://keras.io/api/layers/attention_layers/multi_head_attention/) layer implemented in Keras, use this masking logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    decoder_token_ids -- (n, m) matrix\n",
    "    mask -- (n, 1, m) binary tensor\n",
    "    \"\"\"    \n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "    # add extra dimensions to add the padding\n",
    "    return seq[:, tf.newaxis, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n",
      "tf.Tensor(\n",
      "[[[1. 1. 0. 0. 1.]]\n",
      "\n",
      " [[1. 1. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 1.]]], shape=(3, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[7., 6., 0., 0., 1.],\n",
    "                 [1., 2., 3., 0., 0.],\n",
    "                 [0., 0., 0., 4., 5.]])\n",
    "print(x.shape)\n",
    "print(create_padding_mask(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we multiply (1 - mask) by -1e9 and add it to the sample input sequences, the zeros are essentially set to negative infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = tf.keras.activations.softmax(x)\n",
    "# clean = tf.keras.activations.softmax(x + (1 - create_padding_mask(x)) * -1.0e9)\n",
    "# print(raw.numpy().round(2))\n",
    "# print(clean.numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look-ahead Mask\n",
    "\n",
    "The look-ahead mask follows similar intuition. In training, we will have access to the complete correct output of your training example. The look-ahead mask helps our model pretend that it correctly predicted a part of the output and see if, *without looking ahead*, it can correctly predict the next output. \n",
    "\n",
    "For example, if the expected correct output is `[1, 2, 3]` and we wanted to see if given that the model correctly predicted the first value it could predict the second value, we would mask out the second and third values. So our input would be the masked sequence `[1, -1e9, -1e9]` and see if it could generate `[1, 2, -1e9]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Returns an lower triangular matrix filled with ones\n",
    "    sequence_length -- matrix size\n",
    "    mask -- (size, size) tensor\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3) tf.Tensor(\n",
      "[[[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 1. 1.]]], shape=(1, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "print(temp.shape, temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Self-Attention\n",
    "\n",
    "As the authors of the Transformers paper state, \"Attention is All You Need\". \n",
    "\n",
    "<img src=\"images/self-attention.png\" alt=\"Encoder\" width=\"600\"/>\n",
    "<caption><center><font color='purple'><b>Self-Attention calculation visualization</font></center></caption>\n",
    "    \n",
    "The use of self-attention paired with traditional convolutional networks processing allows for parallelization which speeds up training. We will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to returns rich, attention-based vector representations of the words in our sequence. This type of self-attention can be mathematically expressed as:\n",
    "$$\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask you choose to apply \n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q -- query shape == (..., seq_len_q, depth)\n",
    "        k -- key   shape == (..., seq_len_k, depth)\n",
    "        v -- value shape == (..., seq_len_v, depth_v) where seq_len_k = seq_len_v.\n",
    "        mask: Float tensor with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    matmul_qk = tf.matmul(q,k.T)                    # (..., seq_len_q, seq_len_k)\n",
    "    dk = k.shape[-1]\n",
    "    \n",
    "    # scake the tensor\n",
    "    scaled_attention_logits = matmul_qk/np.sqrt(dk) # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # add the mask\n",
    "    if mask is not None: \n",
    "        scaled_attention_logits += (1.-mask)*(-1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so scores add to 1\n",
    "    attention_weights = tf.keras.activations.softmax(scaled_attention_logits, \n",
    "                                                     axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)         # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = np.array([[1, 0, 1, 1], [0, 1, 1, 1], [1, 0, 0, 1]]).astype(np.float32)\n",
    "# k = np.array([[1, 1, 0, 1], [1, 0, 1, 1 ], [0, 1, 1, 0], [0, 0, 0, 1]]).astype(np.float32)\n",
    "# v = np.array([[0, 0], [1, 0], [1, 0], [1, 1]]).astype(np.float32)\n",
    "# print(q.shape, k.shape, v.shape)\n",
    "\n",
    "# matmul_qk = tf.matmul(q,k.T)\n",
    "# print(matmul_qk.shape)\n",
    "# print(matmul_qk)\n",
    "\n",
    "# dk = k.shape[-1]\n",
    "# np.sqrt(dk) \n",
    "\n",
    "# scaled_attention_logits = matmul_qk/np.sqrt(dk)\n",
    "# print(scaled_attention_logits.shape)\n",
    "# print(scaled_attention_logits)\n",
    "\n",
    "# attention_weights = tf.keras.activations.softmax(scaled_attention_logits, axis=-1)\n",
    "# print(attention_weights.shape)\n",
    "# print(attention_weights)\n",
    "\n",
    "# output = tf.matmul(attention_weights, v)\n",
    "# print(output.shape)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Encoder\n",
    "\n",
    "Transformer Encoder layer pairs **self-attention** and **convolutional neural network style of processing** to improve the speed of training and passes K and V matrices to the Decoder. First, we will implement the Encoder by pairing multi-head attention and a feed forward neural network \n",
    "* `MultiHeadAttention` computes the self-attention several times to detect different features. \n",
    "* Feed forward neural network contains two Dense layers which we'll implement as the function `FullyConnected`\n",
    "\n",
    "The input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n",
    "\n",
    "<img src=\"images/encoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>Transformer encoder layer</font></center></caption></img>\n",
    "   \n",
    "References:\n",
    "\n",
    "* For the `MultiHeadAttention` layer, we use the [Keras implementation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). *Note that if query, key and value are the same, then this function performs self-attention* - **i.e. mha(x,x,x)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The call arguments for `self.mha` are (Where B is for batch_size, T is for target sequence shapes, and S is output_shape):\n",
    " - `query`: Query Tensor of shape (B, T, dim).\n",
    " - `value`: Value Tensor of shape (B, S, dim).\n",
    " - `key`: Optional key Tensor of shape (B, S, dim). If not given, will use value for both key and value, which is the most common case.\n",
    " - `attention_mask`: a boolean mask of shape (B, T, S), that prevents attention to certain positions. The boolean mask specifies which query elements can attend to which key elements, 1 indicates attention and 0 indicates no attention. Broadcasting can happen for the missing batch dimensions and the head dimension.\n",
    " - `return_attention_scores`: A boolean to indicate whether the output should be attention output if True, or (attention_output, attention_scores) if False. Defaults to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(embedding_dim)                            # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This archirecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim,\n",
    "                                      dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            training -- Boolean, set to True to activate training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that padding is not treated as part of input\n",
    "        Returns:\n",
    "            encoder_layer_out -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \"\"\"\n",
    "        # Calculate Self-attention using mha layer\n",
    "        attn_output = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # Apply layer normalization on sum of input and  attention output\n",
    "        out1 = self.layernorm1(x+attn_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # Pass output of multi-head attention layer through a ffn\n",
    "        ffn_output = self.ffn(out1)            # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # Apply dropout layer\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training)\n",
    "        \n",
    "        # Apply layer normalization on sum of o/p from multi-head attention and ffn\n",
    "        encoder_layer_out = self.layernorm2(out1+ffn_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        print('--- Encoder layer ---')\n",
    "        print('X dim: ', x.shape)\n",
    "        print('Attn_output dim: ', attn_output.shape)\n",
    "        print('Norm1 dim: ', out1.shape)\n",
    "        print('FFN_out dim: ', ffn_output.shape)\n",
    "        print('Encoder_layer_out dim: ', encoder_layer_out.shape)\n",
    "        \n",
    "        return encoder_layer_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q dim:  (1, 3, 4)\n",
      "--- Encoder layer ---\n",
      "X dim:  (1, 3, 4)\n",
      "Attn_output dim:  (1, 3, 4)\n",
      "Norm1 dim:  (1, 3, 4)\n",
      "FFN_out dim:  (1, 3, 4)\n",
      "Encoder_layer_out dim:  (1, 3, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=\n",
       "array([[[ 0.23017097, -0.98100376, -0.7870757 ,  1.5379084 ],\n",
       "        [-1.2280798 ,  0.76477593, -0.7169285 ,  1.1802324 ],\n",
       "        [ 0.14880149, -0.48318025, -1.19084   ,  1.525219  ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimension Test\n",
    "q = np.array([[[1, 0, 1, 1],\n",
    "               [0, 1, 1, 1],\n",
    "               [1, 0, 0, 1]]]).astype(np.float32)\n",
    "print('Q dim: ', q.shape)\n",
    "\n",
    "# Initializing Class instance\n",
    "encoder_layer1 = EncoderLayer(4, 2, 8)\n",
    "\n",
    "# Calling Class instance\n",
    "tf.random.set_seed(10)\n",
    "encoded = encoder_layer1(q, True, np.array([[1, 0, 1]]))     # NOTE - This is the call function\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Full Encoder\n",
    "\n",
    "Now we're ready to build the full Transformer Encoder, where we will embedd our input and add the positional encodings we calculated. We will then feed your encoded embeddings to a stack of Encoder layers. \n",
    "\n",
    "<img src=\"images/encoder.png\" alt=\"Encoder\" width=\"330\"/>\n",
    "<caption><center><font color='purple'><b>Transformer Encoder</font></center></caption></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"  \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.embedding_dim)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \"\"\"\n",
    "        # mask, to match the dimension of pos_encoding (5) to input (3)\n",
    "        # used in pos_encoding step down below\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        print('--- Encoder ---')\n",
    "        print('X shape: ', x.shape)\n",
    "        \n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        print('After Embedding: ', x.shape)\n",
    "        \n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= np.sqrt(tf.cast(x.shape[2], dtype=tf.float32))\n",
    "        \n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[0,:seq_len,:]\n",
    "        print('After Positional Encoding: ', x.shape)\n",
    "        \n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        print('After Dropout: ', x.shape)\n",
    "        \n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        print('After Encoding stack: ', x.shape)\n",
    "        \n",
    "        return x  # (batch_size, input_seq_len, fully_connected_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Encoder ---\n",
      "X shape:  (2, 3)\n",
      "After Embedding:  (2, 3, 4)\n",
      "After Positional Encoding:  (2, 3, 4)\n",
      "After Dropout:  (2, 3, 4)\n",
      "--- Encoder layer ---\n",
      "X dim:  (2, 3, 4)\n",
      "Attn_output dim:  (2, 3, 4)\n",
      "Norm1 dim:  (2, 3, 4)\n",
      "FFN_out dim:  (2, 3, 4)\n",
      "Encoder_layer_out dim:  (2, 3, 4)\n",
      "--- Encoder layer ---\n",
      "X dim:  (2, 3, 4)\n",
      "Attn_output dim:  (2, 3, 4)\n",
      "Norm1 dim:  (2, 3, 4)\n",
      "FFN_out dim:  (2, 3, 4)\n",
      "Encoder_layer_out dim:  (2, 3, 4)\n",
      "After Encoding stack:  (2, 3, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[-0.69061005,  1.098871  , -1.2605861 ,  0.8523252 ],\n",
       "        [ 0.73192257, -0.3826024 , -1.4507655 ,  1.1014453 ],\n",
       "        [ 1.0995711 , -1.1686685 , -0.8088867 ,  0.8779842 ]],\n",
       "\n",
       "       [[-0.46129382,  1.069736  , -1.4127711 ,  0.8043292 ],\n",
       "        [ 0.2702725 ,  0.28793636, -1.637089  ,  1.0788802 ],\n",
       "        [ 1.2370992 , -1.0687275 , -0.89450395,  0.7261322 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(10)\n",
    "embedding_dim=4\n",
    "encoderq = Encoder(num_layers=2,\n",
    "                  embedding_dim=embedding_dim,\n",
    "                  num_heads=2,\n",
    "                  fully_connected_dim=8,\n",
    "                  input_vocab_size=32,\n",
    "                  maximum_position_encoding=5)\n",
    "x = np.array([[2, 1, 3], [1, 2, 0]])\n",
    "\n",
    "encoderq_output = encoderq(x, True, None)\n",
    "encoderq_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Decoder\n",
    "\n",
    "The Decoder layer comprises two multi-head attention layers. It takes the K and V matrices generated by the Encoder and computes the second multi-head attention layer with the Q matrix from the output.\n",
    "* The first two blocks are fairly similar to the EncoderLayer except we will return `attention_scores` when computing self-attention\n",
    "\n",
    "<img src=\"images/decoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>Transformer Decoder layer</font></center></caption></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks, \n",
    "    one that takes the new input and uses self-attention, and the other \n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block. \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim,\n",
    "                                      dropout=dropout_rate)\n",
    "\n",
    "        self.mha2 = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim,\n",
    "                                      dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            enc_output -- Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            out3 -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attn_weights_block1 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "            attn_weights_block2 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # shape = (batch_size, input_seq_len, [fully_connected_dim/d_model])\n",
    "        \n",
    "        # BLOCK 1\n",
    "        # Calculate self-attention (return attention scores) + LayerNorm\n",
    "        # Dropout will be applied during training\n",
    "        mult_attn_out1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask, \n",
    "                                                        return_attention_scores=True)  \n",
    "        Q1 = self.layernorm1(mult_attn_out1 + x)\n",
    "        \n",
    "        # BLOCK 2\n",
    "        # Calculate self-attention using the Q (first block) and K and V from the encoder output. \n",
    "        # Dropout will be applied during training\n",
    "        # Apply layer normalization (layernorm2)\n",
    "        mult_attn_out2, attn_weights_block2 = self.mha2(Q1, enc_output, \n",
    "                                                        enc_output,padding_mask, \n",
    "                                                        return_attention_scores=True)\n",
    "        mult_attn_out2 = self.layernorm2(mult_attn_out2 + Q1)\n",
    "           \n",
    "        #BLOCK 3\n",
    "        # Pass through a FFN + Apply Dropput + Layer Norm\n",
    "        ffn_output = self.ffn(mult_attn_out2)  \n",
    "        ffn_output = self.dropout_ffn(ffn_output, training)\n",
    "        out3 = self.layernorm3(ffn_output + mult_attn_out2) \n",
    "        \n",
    "        print('--- Decoder Layer ---')\n",
    "        print('X shape: ', x.shape)\n",
    "        print('Multi_Attn_Op1 shape: ', mult_attn_out1.shape)\n",
    "        print('Q1 shape: ', Q1.shape)\n",
    "        print('Multi_Attn_Op2 shape: ', mult_attn_out2.shape)\n",
    "        print('FFN shape: ', ffn_output.shape)\n",
    "        print('Out3 shape: ', out3.shape)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decoder Layer ---\n",
      "X shape:  (1, 3, 4)\n",
      "Multi_Attn_Op1 shape:  (1, 3, 4)\n",
      "Q1 shape:  (1, 3, 4)\n",
      "Multi_Attn_Op2 shape:  (1, 3, 4)\n",
      "FFN shape:  (1, 3, 4)\n",
      "Out3 shape:  (1, 3, 4)\n",
      "tf.Tensor(\n",
      "[[[-0.22109613 -1.5455484   0.85269207  0.9139525 ]\n",
      "  [-1.6134266   0.582021    1.0286384   0.00276712]\n",
      "  [ 1.2321432  -0.8184075  -1.1359645   0.7222289 ]]], shape=(1, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Dimension Test\n",
    "num_heads=8\n",
    "tf.random.set_seed(10)\n",
    "decoderLayerq = DecoderLayer(\n",
    "    embedding_dim=4, \n",
    "    num_heads=num_heads,\n",
    "    fully_connected_dim=32, \n",
    "    dropout_rate=0.1, \n",
    "    layernorm_eps=1e-6)\n",
    "encoderq_output = tf.constant([[[-0.40172306,  0.11519244, -1.2322885,   1.5188192 ],\n",
    "                               [ 0.4017268,   0.33922842, -1.6836855,   0.9427304 ],\n",
    "                               [ 0.4685002,  -1.6252842,   0.09368491,  1.063099  ]]])\n",
    "q = np.array([[[1, 0, 1, 1], \n",
    "               [0, 1, 1, 1], \n",
    "               [1, 0, 0, 1]]]).astype(np.float32)\n",
    "look_ahead_mask = create_look_ahead_mask(q.shape[1])\n",
    "padding_mask = None\n",
    "\n",
    "out, attn_w_b1, attn_w_b2 = decoderLayerq(q, encoderq_output, True, look_ahead_mask, padding_mask)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Full Decoder\n",
    "\n",
    "<img src=\"images/decoder.png\" alt=\"Encoder\" width=\"300\"/>\n",
    "<caption><center><font color='purple'><b>Transformer Decoder</font></center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the 'target' input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            enc_output --  Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        print('--- Decoder ---')\n",
    "        print('Encoder output shape: ', enc_output.shape)\n",
    "        print('X shape: ', x.shape)\n",
    "        \n",
    "        # Create word embeddings \n",
    "        x = self.embedding(x)                 \n",
    "        print('After Embedding: ', x.shape)\n",
    "        \n",
    "        # Scale embeddings\n",
    "        x *= np.sqrt(tf.cast(x.shape[2], dtype=tf.float32))\n",
    "        \n",
    "        # Add Positional encodings\n",
    "        x += self.pos_encoding[0,:seq_len,:]\n",
    "        print('After Positional Encoding: ', x.shape)\n",
    "        \n",
    "        # Apply a dropout layer\n",
    "        x = self.dropout(x, training)\n",
    "\n",
    "        # A stack of decoder layers and update attention_weights\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, \n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "\n",
    "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
    "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
    "\n",
    "        \n",
    "        # x.shape == (batch_size, target_seq_len, fully_connected_dim)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decoder ---\n",
      "Encoder output shape:  (2, 3, 4)\n",
      "X shape:  (2, 3)\n",
      "After Embedding:  (2, 3, 4)\n",
      "After Positional Encoding:  (2, 3, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (2, 3, 4)\n",
      "Multi_Attn_Op1 shape:  (2, 3, 4)\n",
      "Q1 shape:  (2, 3, 4)\n",
      "Multi_Attn_Op2 shape:  (2, 3, 4)\n",
      "FFN shape:  (2, 3, 4)\n",
      "Out3 shape:  (2, 3, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (2, 3, 4)\n",
      "Multi_Attn_Op1 shape:  (2, 3, 4)\n",
      "Q1 shape:  (2, 3, 4)\n",
      "Multi_Attn_Op2 shape:  (2, 3, 4)\n",
      "FFN shape:  (2, 3, 4)\n",
      "Out3 shape:  (2, 3, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (2, 3, 4)\n",
      "Multi_Attn_Op1 shape:  (2, 3, 4)\n",
      "Q1 shape:  (2, 3, 4)\n",
      "Multi_Attn_Op2 shape:  (2, 3, 4)\n",
      "FFN shape:  (2, 3, 4)\n",
      "Out3 shape:  (2, 3, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (2, 3, 4)\n",
      "Multi_Attn_Op1 shape:  (2, 3, 4)\n",
      "Q1 shape:  (2, 3, 4)\n",
      "Multi_Attn_Op2 shape:  (2, 3, 4)\n",
      "FFN shape:  (2, 3, 4)\n",
      "Out3 shape:  (2, 3, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (2, 3, 4)\n",
      "Multi_Attn_Op1 shape:  (2, 3, 4)\n",
      "Q1 shape:  (2, 3, 4)\n",
      "Multi_Attn_Op2 shape:  (2, 3, 4)\n",
      "FFN shape:  (2, 3, 4)\n",
      "Out3 shape:  (2, 3, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (2, 3, 4)\n",
      "Multi_Attn_Op1 shape:  (2, 3, 4)\n",
      "Q1 shape:  (2, 3, 4)\n",
      "Multi_Attn_Op2 shape:  (2, 3, 4)\n",
      "FFN shape:  (2, 3, 4)\n",
      "Out3 shape:  (2, 3, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (2, 3, 4)\n",
      "Multi_Attn_Op1 shape:  (2, 3, 4)\n",
      "Q1 shape:  (2, 3, 4)\n",
      "Multi_Attn_Op2 shape:  (2, 3, 4)\n",
      "FFN shape:  (2, 3, 4)\n",
      "Out3 shape:  (2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "num_layers=7                 # Number of times decoder network is run\n",
    "embedding_dim=4              # Length for word embedding, to positional encoding, \n",
    "                             # to MHA (Size of each attention head for query and key)\n",
    "                             # Fully Connected Layer ()\n",
    "num_heads=3                  # Number of attention heads - parameter to Multi-Head Attention Layer\n",
    "fully_connected_dim=8        # Nodes in 1st Dense layer of FFN\n",
    "maximum_position_encoding=6  # should be >= embedding_dim\n",
    "target_vocab_size=33\n",
    "\n",
    "x = np.array([[3, 2, 1], \n",
    "              [2, 1, 0]])\n",
    "encoderq_output = tf.constant([[[-0.40172306,  0.11519244, -1.2322885,   1.5188192 ],\n",
    "                                [ 0.4017268,   0.33922842, -1.6836855,   0.9427304 ],\n",
    "                                [ 0.4685002,  -1.6252842,   0.09368491,  1.063099  ]],\n",
    "                               [[-0.3489219,   0.31335592, -1.3568854,   1.3924513 ],\n",
    "                                [-0.08761203, -0.1680029,  -1.2742313,   1.5298463 ],\n",
    "                                [ 0.2627198,  -1.6140151,   0.2212624 ,  1.130033  ]]])\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(x.shape[1])\n",
    "decoderk = Decoder(num_layers,\n",
    "                   embedding_dim, \n",
    "                   num_heads, \n",
    "                   fully_connected_dim,\n",
    "                   target_vocab_size,\n",
    "                   maximum_position_encoding)\n",
    "\n",
    "outd, att_weights = decoderk(x, encoderq_output, False, look_ahead_mask, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8 Transformer\n",
    "The flow of data through the Transformer Architecture is as follows:\n",
    "* First input passes through an Encoder, which is just repeated Encoder layers:\n",
    "    - embedding and positional encoding of input\n",
    "    - multi-head attention on the input\n",
    "    - feed forward neural network to help detect features\n",
    "* Then the predicted output passes through a Decoder, consisting of the decoder layers:\n",
    "    - embedding and positional encoding of the output\n",
    "    - multi-head attention on the generated output\n",
    "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
    "    - a feed forward neural network to help detect features\n",
    "* Finally, after the Nth Decoder layer, one dense layer and a softmax are applied to generate prediction for the next output in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
    "               target_vocab_size, max_positional_encoding_input,\n",
    "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_input,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size, \n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            input_sentence -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            output_sentence -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            enc_padding_mask -- Boolean mask to ensure padding is not treated as part of input\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            dec_padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        \n",
    "        Returns:\n",
    "            final_output --\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Run Encoder - Input Sentence\n",
    "        enc_output = self.encoder(input_sentence, training, enc_padding_mask)\n",
    "        \n",
    "        # Run Decoder - Output Sentence + Encoder Input\n",
    "        dec_output, attention_weights = self.decoder(output_sentence, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        # pass decoder output through a linear layer and softmax\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        print(' --- Transformer ---')\n",
    "        print('Encoder output :', enc_output.shape)\n",
    "        print('Decoder output :', enc_output.shape)\n",
    "        print('Final output :', final_output.shape)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Encoder ---\n",
      "X shape:  (1, 5)\n",
      "After Embedding:  (1, 5, 4)\n",
      "After Positional Encoding:  (1, 5, 4)\n",
      "After Dropout:  (1, 5, 4)\n",
      "--- Encoder layer ---\n",
      "X dim:  (1, 5, 4)\n",
      "Attn_output dim:  (1, 5, 4)\n",
      "Norm1 dim:  (1, 5, 4)\n",
      "FFN_out dim:  (1, 5, 4)\n",
      "Encoder_layer_out dim:  (1, 5, 4)\n",
      "--- Encoder layer ---\n",
      "X dim:  (1, 5, 4)\n",
      "Attn_output dim:  (1, 5, 4)\n",
      "Norm1 dim:  (1, 5, 4)\n",
      "FFN_out dim:  (1, 5, 4)\n",
      "Encoder_layer_out dim:  (1, 5, 4)\n",
      "--- Encoder layer ---\n",
      "X dim:  (1, 5, 4)\n",
      "Attn_output dim:  (1, 5, 4)\n",
      "Norm1 dim:  (1, 5, 4)\n",
      "FFN_out dim:  (1, 5, 4)\n",
      "Encoder_layer_out dim:  (1, 5, 4)\n",
      "--- Encoder layer ---\n",
      "X dim:  (1, 5, 4)\n",
      "Attn_output dim:  (1, 5, 4)\n",
      "Norm1 dim:  (1, 5, 4)\n",
      "FFN_out dim:  (1, 5, 4)\n",
      "Encoder_layer_out dim:  (1, 5, 4)\n",
      "--- Encoder layer ---\n",
      "X dim:  (1, 5, 4)\n",
      "Attn_output dim:  (1, 5, 4)\n",
      "Norm1 dim:  (1, 5, 4)\n",
      "FFN_out dim:  (1, 5, 4)\n",
      "Encoder_layer_out dim:  (1, 5, 4)\n",
      "--- Encoder layer ---\n",
      "X dim:  (1, 5, 4)\n",
      "Attn_output dim:  (1, 5, 4)\n",
      "Norm1 dim:  (1, 5, 4)\n",
      "FFN_out dim:  (1, 5, 4)\n",
      "Encoder_layer_out dim:  (1, 5, 4)\n",
      "After Encoding stack:  (1, 5, 4)\n",
      "--- Decoder ---\n",
      "Encoder output shape:  (1, 5, 4)\n",
      "X shape:  (1, 5)\n",
      "After Embedding:  (1, 5, 4)\n",
      "After Positional Encoding:  (1, 5, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (1, 5, 4)\n",
      "Multi_Attn_Op1 shape:  (1, 5, 4)\n",
      "Q1 shape:  (1, 5, 4)\n",
      "Multi_Attn_Op2 shape:  (1, 5, 4)\n",
      "FFN shape:  (1, 5, 4)\n",
      "Out3 shape:  (1, 5, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (1, 5, 4)\n",
      "Multi_Attn_Op1 shape:  (1, 5, 4)\n",
      "Q1 shape:  (1, 5, 4)\n",
      "Multi_Attn_Op2 shape:  (1, 5, 4)\n",
      "FFN shape:  (1, 5, 4)\n",
      "Out3 shape:  (1, 5, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (1, 5, 4)\n",
      "Multi_Attn_Op1 shape:  (1, 5, 4)\n",
      "Q1 shape:  (1, 5, 4)\n",
      "Multi_Attn_Op2 shape:  (1, 5, 4)\n",
      "FFN shape:  (1, 5, 4)\n",
      "Out3 shape:  (1, 5, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (1, 5, 4)\n",
      "Multi_Attn_Op1 shape:  (1, 5, 4)\n",
      "Q1 shape:  (1, 5, 4)\n",
      "Multi_Attn_Op2 shape:  (1, 5, 4)\n",
      "FFN shape:  (1, 5, 4)\n",
      "Out3 shape:  (1, 5, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (1, 5, 4)\n",
      "Multi_Attn_Op1 shape:  (1, 5, 4)\n",
      "Q1 shape:  (1, 5, 4)\n",
      "Multi_Attn_Op2 shape:  (1, 5, 4)\n",
      "FFN shape:  (1, 5, 4)\n",
      "Out3 shape:  (1, 5, 4)\n",
      "--- Decoder Layer ---\n",
      "X shape:  (1, 5, 4)\n",
      "Multi_Attn_Op1 shape:  (1, 5, 4)\n",
      "Q1 shape:  (1, 5, 4)\n",
      "Multi_Attn_Op2 shape:  (1, 5, 4)\n",
      "FFN shape:  (1, 5, 4)\n",
      "Out3 shape:  (1, 5, 4)\n",
      " --- Transformer ---\n",
      "Encoder output : (1, 5, 4)\n",
      "Decoder output : (1, 5, 4)\n",
      "Final output : (1, 5, 35)\n"
     ]
    }
   ],
   "source": [
    "# Dimension Test\n",
    "num_layers = 6\n",
    "embedding_dim = 4\n",
    "num_heads = 4\n",
    "fully_connected_dim = 8\n",
    "input_vocab_size = 30\n",
    "target_vocab_size = 35\n",
    "max_positional_encoding_input = 5\n",
    "max_positional_encoding_target = 6\n",
    "\n",
    "trans = Transformer(num_layers, \n",
    "                    embedding_dim, \n",
    "                    num_heads, \n",
    "                    fully_connected_dim, \n",
    "                    input_vocab_size, \n",
    "                    target_vocab_size, \n",
    "                    max_positional_encoding_input,\n",
    "                    max_positional_encoding_target)\n",
    "# 0 is the padding value\n",
    "sentence_lang_a = np.array([[2, 1, 4, 3, 0]])\n",
    "sentence_lang_b = np.array([[3, 2, 1, 0, 0]])\n",
    "\n",
    "enc_padding_mask = create_padding_mask(sentence_lang_a)\n",
    "dec_padding_mask = create_padding_mask(sentence_lang_b)\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(sentence_lang_a.shape[1])\n",
    "\n",
    "translation, weights = trans(\n",
    "    sentence_lang_a,\n",
    "    sentence_lang_b,\n",
    "    True,  # Training\n",
    "    enc_padding_mask,\n",
    "    look_ahead_mask,\n",
    "    dec_padding_mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
