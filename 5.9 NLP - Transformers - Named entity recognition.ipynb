{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network Application: Named-Entity Recognition\n",
    "\n",
    "When faced with a large amount of unstructured text data, named-entity recognition (NER) can help you detect and classify important information in your dataset. For instance, in the running example \"Jane vists Africa in September\", NER would help detect \"Jane\", \"Africa\", and \"September\" as named-entities and classify them as person, location, and time. \n",
    "\n",
    "<font color='blue'>Approach:  \n",
    "* Use tokenizers and pre-trained models from the HuggingFace Library.\n",
    "* Fine-tune a pre-trained transformer model for Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uditg\\anaconda3\\envs\\coursera\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "# import random\n",
    "# import logging\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset of resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/nlp_ner/ner.json\", lines=True)\n",
    "df = df.drop(['extras'], axis=1)\n",
    "df['content'] = df['content'].str.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Abhishek Jha Application Development Associate...   \n",
       "1  Afreen Jamadar Active member of IIIT Committee...   \n",
       "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
       "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
       "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
       "\n",
       "                                          annotation  \n",
       "0  [{'label': ['Skills'], 'points': [{'start': 12...  \n",
       "1  [{'label': ['Email Address'], 'points': [{'sta...  \n",
       "2  [{'label': ['Skills'], 'points': [{'start': 37...  \n",
       "3  [{'label': ['Skills'], 'points': [{'start': 80...  \n",
       "4  [{'label': ['Degree'], 'points': [{'start': 20...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  â€¢ To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possibl\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'][0][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': ['Skills'],\n",
       "  'points': [{'start': 1295,\n",
       "    'end': 1621,\n",
       "    'text': '\\nâ€¢ Programming language: C, C++, Java\\nâ€¢ Oracle PeopleSoft\\nâ€¢ Internet Of Things\\nâ€¢ Machine Learning\\nâ€¢ Database Management System\\nâ€¢ Computer Networks\\nâ€¢ Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\nâ€¢ Honest and Hard-Working\\nâ€¢ Tolerant and Flexible to Different Situations\\nâ€¢ Polite and Calm\\nâ€¢ Team-Player'}]},\n",
       " {'label': ['Skills'],\n",
       "  'points': [{'start': 993,\n",
       "    'end': 1153,\n",
       "    'text': 'C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)'}]},\n",
       " {'label': ['College Name'],\n",
       "  'points': [{'start': 939, 'end': 956, 'text': 'Kendriya Vidyalaya'}]},\n",
       " {'label': ['College Name'],\n",
       "  'points': [{'start': 883, 'end': 904, 'text': 'Woodbine modern school'}]},\n",
       " {'label': ['Graduation Year'],\n",
       "  'points': [{'start': 856, 'end': 860, 'text': '2017\\n'}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['annotation'][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Skills']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['annotation'][0][1]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing - to create 'Entities' column, which is an ordered list of  NERs based on data in 'Annotation' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeIntervals(intervals):\n",
    "    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "\n",
    "    for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            if higher[0] <= lower[1]:\n",
    "                if lower[2] is higher[2]:\n",
    "                    upper_bound = max(lower[1], higher[1])\n",
    "                    merged[-1] = (lower[0], upper_bound, lower[2])\n",
    "                else:\n",
    "                    if lower[1] > higher[1]:\n",
    "                        merged[-1] = lower\n",
    "                    else:\n",
    "                        merged[-1] = (lower[0], higher[1], higher[2])\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(df):\n",
    "    entities = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        entity = []\n",
    "    \n",
    "        for annot in df['annotation'][i]:\n",
    "            try:\n",
    "                ent = annot['label'][0]\n",
    "                start = annot['points'][0]['start']\n",
    "                end = annot['points'][0]['end'] + 1\n",
    "                entity.append((start, end, ent))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        entity = mergeIntervals(entity)\n",
    "        entities.append(entity)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "      <td>[(0, 12, Name), (13, 46, Designation), (49, 58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>[(0, 14, Name), (62, 68, Location), (104, 148,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>[(0, 21, Name), (22, 31, Location), (65, 117, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>[(0, 12, Name), (13, 51, Designation), (54, 60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "      <td>[(0, 13, Name), (14, 22, Designation), (24, 41...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Abhishek Jha Application Development Associate...   \n",
       "1  Afreen Jamadar Active member of IIIT Committee...   \n",
       "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
       "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
       "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
       "\n",
       "                                          annotation  \\\n",
       "0  [{'label': ['Skills'], 'points': [{'start': 12...   \n",
       "1  [{'label': ['Email Address'], 'points': [{'sta...   \n",
       "2  [{'label': ['Skills'], 'points': [{'start': 37...   \n",
       "3  [{'label': ['Skills'], 'points': [{'start': 80...   \n",
       "4  [{'label': ['Degree'], 'points': [{'start': 20...   \n",
       "\n",
       "                                            entities  \n",
       "0  [(0, 12, Name), (13, 46, Designation), (49, 58...  \n",
       "1  [(0, 14, Name), (62, 68, Location), (104, 148,...  \n",
       "2  [(0, 21, Name), (22, 31, Location), (65, 117, ...  \n",
       "3  [(0, 12, Name), (13, 51, Designation), (54, 60...  \n",
       "4  [(0, 13, Name), (14, 22, Designation), (24, 41...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entities'] = get_entities(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 12, 'Name'),\n",
       " (13, 46, 'Designation'),\n",
       " (49, 58, 'Companies worked at'),\n",
       " (60, 69, 'Location'),\n",
       " (95, 146, 'Email Address'),\n",
       " (372, 405, 'Designation'),\n",
       " (407, 416, 'Companies worked at'),\n",
       " (727, 770, 'Designation'),\n",
       " (771, 814, 'College Name'),\n",
       " (856, 861, 'Graduation Year'),\n",
       " (883, 905, 'College Name'),\n",
       " (939, 957, 'College Name'),\n",
       " (993, 1154, 'Skills'),\n",
       " (1295, 1622, 'Skills')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entities'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences `->` NER English Tags `->` NER Integer Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, 'r', encoding='utf8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content'].replace(\"\\n\", \" \")\n",
    "            entities = []\n",
    "            data_annotations = data['annotation']\n",
    "            if data_annotations is not None:\n",
    "                for annotation in data_annotations:\n",
    "                    #only a single point in text annotation.\n",
    "                    point = annotation['points'][0]\n",
    "                    labels = annotation['label']\n",
    "                    # handle both list of labels or a single label.\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    for label in labels:\n",
    "                        point_start = point['start']\n",
    "                        point_end = point['end']\n",
    "                        point_text = point['text']\n",
    "                        \n",
    "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                        if lstrip_diff != 0:\n",
    "                            point_start = point_start + lstrip_diff\n",
    "                        if rstrip_diff != 0:\n",
    "                            point_end = point_end - rstrip_diff\n",
    "                        entities.append((point_start, point_end + 1 , label))\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trim_entity_spans(convert_dataturks_to_spacy(\"data/nlp_ner/ner.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm - changed from tqdm to list in for loop\n",
    "\n",
    "def clean_dataset(data):\n",
    "    cleanedDF = pd.DataFrame(columns=[\"setences_cleaned\"])\n",
    "    sum1 = 0\n",
    "    for i in list(range(len(data))):\n",
    "        start = 0\n",
    "        emptyList = [\"Empty\"] * len(data[i][0].split())\n",
    "        numberOfWords = 0\n",
    "        lenOfString = len(data[i][0])\n",
    "        strData = data[i][0]\n",
    "        strDictData = data[i][1]\n",
    "        lastIndexOfSpace = strData.rfind(' ')\n",
    "        for i in range(lenOfString):\n",
    "            if (strData[i]==\" \" and strData[i+1]!=\" \"):\n",
    "                for k,v in strDictData.items():\n",
    "                    for j in range(len(v)):\n",
    "                        entList = v[len(v)-j-1]\n",
    "                        if (start>=int(entList[0]) and i<=int(entList[1])):\n",
    "                            emptyList[numberOfWords] = entList[2]\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                start = i + 1  \n",
    "                numberOfWords += 1\n",
    "            if (i == lastIndexOfSpace):\n",
    "                for j in range(len(v)):\n",
    "                        entList = v[len(v)-j-1]\n",
    "                        if (lastIndexOfSpace>=int(entList[0]) and lenOfString<=int(entList[1])):\n",
    "                            emptyList[numberOfWords] = entList[2]\n",
    "                            numberOfWords += 1\n",
    "        cleanedDF = cleanedDF.append(pd.Series([emptyList],  index=cleanedDF.columns ), ignore_index=True )\n",
    "        sum1 = sum1 + numberOfWords\n",
    "    return cleanedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDF = clean_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setences_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Name, Name, Empty, Empty, Empty, Empty, Empty...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    setences_cleaned\n",
       "0  [Name, Name, Designation, Designation, Designa...\n",
       "1  [Name, Name, Empty, Empty, Empty, Empty, Empty..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'Name',\n",
       " 'Designation',\n",
       " 'Designation',\n",
       " 'Designation',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedDF['setences_cleaned'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  â€¢ To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possibl\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'][0][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and Generating Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(cleanedDF['setences_cleaned'].explode().unique())\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'College Name',\n",
       " 'Companies worked at',\n",
       " 'Degree',\n",
       " 'Designation',\n",
       " 'Email Address',\n",
       " 'Empty',\n",
       " 'Graduation Year',\n",
       " 'Location',\n",
       " 'Name',\n",
       " 'Skills',\n",
       " 'UNKNOWN',\n",
       " 'Years of Experience'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNKNOWN': 0,\n",
       " 'Companies worked at': 1,\n",
       " 'Location': 2,\n",
       " 'Name': 3,\n",
       " 'Years of Experience': 4,\n",
       " 'Email Address': 5,\n",
       " 'Skills': 6,\n",
       " 'Degree': 7,\n",
       " 'Empty': 8,\n",
       " 'Designation': 9,\n",
       " 'College Name': 10,\n",
       " 'Graduation Year': 11}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "labels = cleanedDF['setences_cleaned'].values.tolist()\n",
    "\n",
    "tags = pad_sequences([[tag2id.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2id[\"Empty\"], padding=\"post\",truncating=\"post\",\n",
    "                     dtype=\"long\")  #value - padding value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 9, ..., 8, 8, 8],\n",
       "       [3, 3, 8, ..., 8, 8, 8],\n",
       "       [3, 3, 3, ..., 8, 6, 8],\n",
       "       ...,\n",
       "       [3, 3, 9, ..., 8, 8, 8],\n",
       "       [3, 3, 9, ..., 8, 8, 8],\n",
       "       [3, 3, 9, ..., 8, 8, 8]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 512)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Align Labels with ðŸ¤— Library\n",
    "\n",
    "Before feeding the texts to a Transformer model, we need to tokenize the input using a [ðŸ¤— Transformer tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html). It is crucial that the tokenizer you use must match the Transformer model type we are using - [DistilBERT fast tokenizer](https://huggingface.co/transformers/model_doc/distilbert.html) in this case, which standardizes the length of sequences to 512 and pads with zeros. Notice this matches the maximum length we used when creating tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast \n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('pretrainedmodel/nlp_transformer/tokenizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer models are often trained by tokenizers that split words into subwords.** For instance, the word 'Africa' might get split into multiple subtokens. This can create some misalignment between the list of tags for the dataset and the list of labels generated by the tokenizer, **since the tokenizer can split one word into several**, or add special tokens. Before processing, it is important that we align the lists of tags and the list of labels generated by the selected tokenizer with a `tokenize_and_align_labels()` function.\n",
    "\n",
    "* The tokenizer cuts sequences that exceed the maximum size allowed by your model with the parameter `truncation=True`\n",
    "* Aligns the list of tags and labels with the tokenizer `word_ids` method returns a list that maps the subtokens to the original word in the sentence and special tokens to `None`. \n",
    "* Set the labels of all the special tokens (`None`) to -100 to prevent them from affecting the loss function. \n",
    "* Label of the first subtoken of a word and set the label for the following subtokens to -100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, examples, tags):\n",
    "    tokenized_inputs = tokenizer(examples, truncation=True, is_split_into_words=False, padding='max_length', max_length=512)\n",
    "    labels = []\n",
    "    for i, label in enumerate(tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so \n",
    "            # they are automatically ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = tokenizer(df['content'].values.tolist(), truncation=True, is_split_into_words=False, padding='max_length', max_length=512)\n",
    "# labels = []\n",
    "\n",
    "# for i, label in enumerate(tags[:1,:]):\n",
    "#     print('original labels: ', label[:10])\n",
    "#     word_ids = temp.word_ids(batch_index=i)\n",
    "#     print('words :', word_ids[:20])\n",
    "#     previous_word_idx = None\n",
    "#     label_ids = []\n",
    "#     for word_idx in word_ids:\n",
    "# #         print('word idx :', word_idx)\n",
    "#         if word_idx is None:\n",
    "#             label_ids.append(-100)\n",
    "#         elif word_idx != previous_word_idx:\n",
    "#             label_ids.append(label[word_idx])\n",
    "#         else:\n",
    "#             label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "#         previous_word_idx = word_idx\n",
    "\n",
    "#     labels.append(label_ids)\n",
    "\n",
    "# temp[\"labels\"] = labels\n",
    "# print('revised labels: ', temp['labels'][0][:15])\n",
    "\n",
    "# # word_ids are like a sequential ids (0, 1, 2, 3) etc.\n",
    "# # revised labels - essentially reshape original labels using idx from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((220,), (220, 512))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'].shape, tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenize_and_align_labels(tokenizer, df['content'].values.tolist(), tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    test['input_ids'],\n",
    "    test['labels']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 220)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['input_ids']), len(test['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['input_ids'][0]), len(test['labels'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Now we can feed our data into into a pretrained ðŸ¤— model. We will optimize a DistilBERT model, which matches the tokenizer we used to preprocess your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForTokenClassification.\n",
      "\n",
      "All the layers of TFDistilBertForTokenClassification were initialized from the model checkpoint at pretrainedmodel/nlp_transformer/tokenizer/DistilBert/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForTokenClassification\n",
    "\n",
    "model = TFDistilBertForTokenClassification.from_pretrained('pretrainedmodel/nlp_transformer/DistilBertTokenClass/',\n",
    "                                                           num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...the usual stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TFTokenClassificationLoss.compute_loss of <transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForTokenClassification object at 0x000001B9A688AF08>>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x1b9872af978>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x1b9872af978>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uditg\\anaconda3\\envs\\coursera\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:493: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "14/14 [==============================] - 222s 16s/step - loss: 1.7078 - accuracy: 0.5794\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 267s 19s/step - loss: 0.6055 - accuracy: 0.7537\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 281s 20s/step - loss: 0.5154 - accuracy: 0.7537\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 258s 18s/step - loss: 0.5033 - accuracy: 0.7537\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 275s 20s/step - loss: 0.4902 - accuracy: 0.7537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b9afdb0488>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy']) # can also use any keras loss fn\n",
    "model.fit(train_dataset.shuffle(1000).batch(16),\n",
    "          epochs=5, \n",
    "          batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to change version of some packages to get the model training to work\n",
    "# scipy==1.4.1\n",
    "# gast==0.3.3\n",
    "# tensorflow-estimator==2.3.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coursera]",
   "language": "python",
   "name": "conda-env-coursera-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
